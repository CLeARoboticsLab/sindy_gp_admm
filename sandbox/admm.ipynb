{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective p = 0.6745532015682119"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shrinkage z = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26021863023068526; 0.5729073986522815; 0.7170654763947101;;]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.9375839696474381 0.0 0.0; 0.340629284695301 0.39189272386963125 0.0; 0.6597315747935305 0.2140180791722751 0.9137190976284412], [0.9375839696474381 0.340629284695301 0.6597315747935305; 0.0 0.39189272386963125 0.2140180791722751; 0.0 0.0 0.9137190976284412])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra \n",
    "\n",
    "# objective \n",
    "function objective(A, b, lambda, x, z) \n",
    "\n",
    "    p = ( 1/2 * sum( ( A*x - b ).^2 ) + lambda*norm(z,1) ) \n",
    "\n",
    "    return p \n",
    "end \n",
    "\n",
    "# test function \n",
    "A = rand(3,3)  \n",
    "b = rand(3,1)  \n",
    "lambda = 0.1  \n",
    "x = rand(3,1)  \n",
    "z = rand(3,1) \n",
    "p = objective(A, b, lambda, x, z) \n",
    "println(\"objective p = \", p)\n",
    "\n",
    "# shrinkage \n",
    "function shrinkage(x, kappa) \n",
    "\n",
    "    z = 0*x ; \n",
    "    for i = 1:length(x) \n",
    "        z[i] = max( 0, x[i] - kappa ) - max( 0, -x[i] - kappa ) \n",
    "    end \n",
    "\n",
    "    return z \n",
    "end \n",
    "\n",
    "# test shrinkage \n",
    "kappa = 0.1 ; \n",
    "z = shrinkage(x, kappa) ; \n",
    "println(\"shrinkage z = \", z)\n",
    "\n",
    "# cache factorization \n",
    "function factor(A, rho)\n",
    "\n",
    "    m,n =  size(A) ; \n",
    "    if m >= n \n",
    "        C = cholesky( A'*A + rho*I ) \n",
    "    else\n",
    "        C = cholesky( I + 1/rho*(A*A') )  \n",
    "    end \n",
    "    L = C.L  \n",
    "    U = C.U \n",
    "\n",
    "    return L, U \n",
    "end \n",
    "\n",
    "# test \n",
    "rho = 0.1 \n",
    "L, U = factor(A, rho) \n",
    "\n",
    "# end \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual ADMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching -(::Int64, ::Matrix{Float64})\nFor element-wise subtraction, use broadcasting with dot syntax: scalar .- array\nClosest candidates are:\n  -(::Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}) at int.jl:85\n  -(::T, !Matched::T) where T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} at int.jl:86\n  -(!Matched::SparseArrays.AbstractSparseMatrixCSC, ::Array) at C:\\Users\\junet\\AppData\\Local\\Programs\\Julia-1.8.3\\share\\julia\\stdlib\\v1.8\\SparseArrays\\src\\sparsematrix.jl:1834\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching -(::Int64, ::Matrix{Float64})\n",
      "For element-wise subtraction, use broadcasting with dot syntax: scalar .- array\n",
      "Closest candidates are:\n",
      "  -(::Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}) at int.jl:85\n",
      "  -(::T, !Matched::T) where T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} at int.jl:86\n",
      "  -(!Matched::SparseArrays.AbstractSparseMatrixCSC, ::Array) at C:\\Users\\junet\\AppData\\Local\\Programs\\Julia-1.8.3\\share\\julia\\stdlib\\v1.8\\SparseArrays\\src\\sparsematrix.jl:1834\n",
      "  ...\n",
      "\n",
      "Stacktrace:\n",
      " [1] lasso_admm(A::Matrix{Float64}, b::Matrix{Float64}, lamda::Float64, rho::Float64, alpha::Float64)\n",
      "   @ Main c:\\Users\\junet\\Documents\\grad_school\\julia_learn\\sandbox\\admm.ipynb:69\n",
      " [2] top-level scope\n",
      "   @ c:\\Users\\junet\\Documents\\grad_school\\julia_learn\\sandbox\\admm.ipynb:93"
     ]
    }
   ],
   "source": [
    "# define output hist struct \n",
    "struct Hist \n",
    "    objval \n",
    "    r_norm \n",
    "    s_norm \n",
    "    eps_pri \n",
    "    eps_dual \n",
    "end \n",
    "hist = Hist([], [], [], [], [])\n",
    "\n",
    "function lasso_admm(A, b, lamda, rho, alpha) \n",
    "# ------------------------------------------------------------------------\n",
    "# lasso  Solve lasso problem via ADMM\n",
    "#\n",
    "# [z, history] = lasso(A, b, lambda, rho, alpha);\n",
    "#\n",
    "# Solves the following problem via ADMM:\n",
    "#\n",
    "#   minimize 1/2*|| Ax - b ||_2^2 + \\lambda || x ||_1\n",
    "#\n",
    "# The solution is returned in the vector x.\n",
    "#\n",
    "# history is a structure that contains:\n",
    "#   objval   = objective function values \n",
    "#   r_norm   = primal residual norms \n",
    "#   s_norm   = dual residual norms \n",
    "#   eps_pri  = tolerances for the primal norms at each iteration\n",
    "#   eps_dual = tolerance for dual residual norms at each iteration\n",
    "#\n",
    "# rho is the augmented Lagrangian parameter.\n",
    "#\n",
    "# alpha is the over-relaxation parameter (typical values for alpha are\n",
    "# between 1.0 and 1.8).\n",
    "# \n",
    "# Reference: \n",
    "# http://www.stanford.edu/~boyd/papers/distr_opt_stat_learning_admm.html\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "    # define constants \n",
    "    max_iter = 1000  \n",
    "    abstol   = 1e-4 \n",
    "    reltol   = 1e-2 \n",
    "\n",
    "    # data pre-processing \n",
    "    m, n = size(A) \n",
    "    Atb = A'*b                          # save matrix-vector multiply \n",
    "\n",
    "    # ADMM solver \n",
    "    x = 0*b  \n",
    "    z = 0*b \n",
    "    u = 0*b \n",
    "\n",
    "    # cache factorization \n",
    "    L, U = factor(A, rho) \n",
    "\n",
    "    # begin iterations \n",
    "    for k = 1:max_iter \n",
    "\n",
    "        # x-update \n",
    "        q = Atb + rho*(z - u)           # temp value \n",
    "        if m >= n                       # if skinny \n",
    "            x = U \\ ( L \\ q ) \n",
    "        else                            # if fat \n",
    "            x = q / rho - ( A' * ( U \\ ( L \\ (A*q) ) ) ) / rho^2 \n",
    "        end \n",
    "\n",
    "        # z-update \n",
    "        z_old = z \n",
    "        x_hat = alpha*x + (1 .- alpha*z_old) \n",
    "        z = shrinkage(x_hat + u, lambda/rho) \n",
    "\n",
    "        # u-update \n",
    "        u = u + (x_hat - z) \n",
    "\n",
    "        # diagnostics + termination checks \n",
    "        p = objective(A, b, lambda, x, z) \n",
    "        push!( hist.objval, p )\n",
    "        push!( hist.r_norm, norm(x - z) )\n",
    "        push!( hist.s_norm, norm( -rho*(z - z_old) ) )\n",
    "        push!( hist.eps_pri, sqrt(n)*abstol + reltol*max(norm(x), norm(-z)) ) \n",
    "        push!( hist.eps_dual, sqrt(n)*abstol + reltol*norm(rho*u) ) \n",
    "\n",
    "        if hist.r_norm[k] < hist.eps_pri[k] && hist.s_norm[k] < hist.eps_dual[k] \n",
    "            break \n",
    "        end \n",
    "\n",
    "    end \n",
    "\n",
    "    return z, hist\n",
    "end \n",
    "\n",
    "# test \n",
    "x, hist = lasso_admm(A, b, lambda, 1.0, 1.0) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
